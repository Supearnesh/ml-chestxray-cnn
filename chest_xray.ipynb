{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "976365bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9da6850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Create a list 'files' of the 5856 downloaded images saved locally\n",
    "xray_files = np.array(glob('chestXrays/train/normal/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8ad251ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chestXrays/train/normal/NORMAL2-IM-0927-0001.jpeg',\n",
       "       'chestXrays/train/normal/NORMAL2-IM-1056-0001.jpeg',\n",
       "       'chestXrays/train/normal/IM-0427-0001.jpeg', ...,\n",
       "       'chestXrays/train/normal/NORMAL2-IM-1011-0001.jpeg',\n",
       "       'chestXrays/train/normal/NORMAL2-IM-0826-0001.jpeg',\n",
       "       'chestXrays/train/normal/NORMAL2-IM-0960-0001.jpeg'], dtype='<U54')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that file names have been stored correctly\n",
    "xray_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1692ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with file names so we can use 'apply()' later to transform images to tensors\n",
    "xray_df = pd.DataFrame(data=xray_files, columns=['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "70f79e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    return tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "80f5d95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chestXrays/train/normal/NORMAL2-IM-0927-0001.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chestXrays/train/normal/NORMAL2-IM-1056-0001.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chestXrays/train/normal/IM-0427-0001.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chestXrays/train/normal/NORMAL2-IM-1260-0001.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chestXrays/train/normal/IM-0656-0001-0001.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name\n",
       "0  chestXrays/train/normal/NORMAL2-IM-0927-0001.jpeg\n",
       "1  chestXrays/train/normal/NORMAL2-IM-1056-0001.jpeg\n",
       "2          chestXrays/train/normal/IM-0427-0001.jpeg\n",
       "3  chestXrays/train/normal/NORMAL2-IM-1260-0001.jpeg\n",
       "4     chestXrays/train/normal/IM-0656-0001-0001.jpeg"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that the dataframe is correctly structured\n",
    "xray_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "70cb279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to transform the images for processing\n",
    "def to_tensor(img_url):\n",
    "    transform = T.Compose([T.Resize(256), T.Resize((256,364)), T.ToTensor()])\n",
    "    return transform(Image.open(img_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e9757bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 364])\n",
      "torch.Size([1, 256, 364])\n",
      "torch.Size([1, 256, 364])\n",
      "torch.Size([1, 256, 364])\n",
      "torch.Size([1, 256, 364])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify tensor shape across a few samples\n",
    "for xray in xray_files[:5]:\n",
    "    print(to_tensor(xray).shape)\n",
    "    \n",
    "to_tensor(xray_files[0]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0aecbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'tensor' with the to_tensor() function applied to 'file_name'\n",
    "xray_df['tensor'] = xray_df['file_name'].apply(to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4fba81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    return tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "21caf225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shape(xray_df['tensor'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e3258738",
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_df['channels'] = xray_df['tensor'].apply(get_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bb5ebd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: file_name, dtype: object)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xray_df[xray_df['channels']>1]['file_name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4c3bd347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files removed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "count = 0\n",
    "\n",
    "for file in xray_df[xray_df['channels']>1]['file_name']:\n",
    "    os.remove(file)\n",
    "    count+=1\n",
    "\n",
    "print('{} files removed.'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bde500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c79fdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/supearnesh/Downloads/chest_xray/all/per...</td>\n",
       "      <td>[[[tensor(0.8157), tensor(0.8078), tensor(0.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/supearnesh/Downloads/chest_xray/all/per...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.0118), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/supearnesh/Downloads/chest_xray/all/per...</td>\n",
       "      <td>[[[tensor(0.1765), tensor(0.1765), tensor(0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/supearnesh/Downloads/chest_xray/all/per...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/supearnesh/Downloads/chest_xray/all/NOR...</td>\n",
       "      <td>[[[tensor(0.0941), tensor(0.1020), tensor(0.10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  /Users/supearnesh/Downloads/chest_xray/all/per...   \n",
       "1  /Users/supearnesh/Downloads/chest_xray/all/per...   \n",
       "2  /Users/supearnesh/Downloads/chest_xray/all/per...   \n",
       "3  /Users/supearnesh/Downloads/chest_xray/all/per...   \n",
       "4  /Users/supearnesh/Downloads/chest_xray/all/NOR...   \n",
       "\n",
       "                                              tensor  \n",
       "0  [[[tensor(0.8157), tensor(0.8078), tensor(0.81...  \n",
       "1  [[[tensor(0.), tensor(0.), tensor(0.0118), ten...  \n",
       "2  [[[tensor(0.1765), tensor(0.1765), tensor(0.17...  \n",
       "3  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "4  [[[tensor(0.0941), tensor(0.1020), tensor(0.10...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the dataframe is correctly created\n",
    "xray_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96d27817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 364])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the shape of the tensors are as expected\n",
    "xray_df['tensor'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6408ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.4925113022327423\n",
      "std: 0.053436361253261566\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of image tensors\n",
    "tensor_vals = []\n",
    "\n",
    "for tensor in xray_df['tensor'][:4]:\n",
    "    tensor_vals.append(tensor.mean())\n",
    "\n",
    "img_mean = np.mean(tensor_vals)\n",
    "img_std = np.std(tensor_vals)\n",
    "    \n",
    "print('mean: {}'.format(img_mean))\n",
    "print('std: {}'.format(img_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60f5aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Looks like multiple labels can be associated with images so we'll have to parse this in the neural network\n",
    "\n",
    "def pre_process_image(img_url):\n",
    "    # Load image from file\n",
    "    img = Image.open(img_url)\n",
    "\n",
    "    # all pre-trained models expect input images normalized in the same way\n",
    "    # i.e. mini-batches of 1-channel grayscale images of shape (1 x H x W)\n",
    "    # H and W are expected to be at least 224\n",
    "    transform = T.Compose([T.Resize(224), T.CenterCrop(224), T.ToTensor()])\n",
    "    transformed_img = transform(img)\n",
    "    \n",
    "    # the images have to be loaded in to a range of [0, 1]\n",
    "    # then normalized using mean = [0.4925113] and std = [0.05343636]\n",
    "    normalize = T.Normalize(mean=img_mean, std=img_std)\n",
    "    normalized_img = normalize(transformed_img)\n",
    "    \n",
    "    # model loading\n",
    "    # tensor_img = normalized_img.unsqueeze(0)\n",
    "    \n",
    "    return normalized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9f81197",
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_df['file_name'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c809ef5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[[tensor(-5.1071), tensor(-4.9603), tensor(-5...\n",
       "1    [[[tensor(-1.7312), tensor(-1.5845), tensor(-1...\n",
       "2    [[[tensor(-7.1619), tensor(-7.1619), tensor(-7...\n",
       "3    [[[tensor(-3.8595), tensor(-3.7127), tensor(-3...\n",
       "4    [[[tensor(-7.1619), tensor(-7.1619), tensor(-7...\n",
       "Name: normalized_tensor, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process_image('chestXrays/all/person604_bacteria_2463.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d7f73cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224, 224])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process_image('chestXrays/all/person604_bacteria_2463.jpeg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f0d1cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Set PIL to be tolerant of image files that are truncated.\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "### DONE: Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor(), T.Normalize(mean=img_mean, std=img_std)])\n",
    "\n",
    "dataset_train = datasets.ImageFolder('/Users/supearnesh/Downloads/chest_xray/train', transform=transform)\n",
    "dataset_val = datasets.ImageFolder('/Users/supearnesh/Downloads/chest_xray/val', transform=transform)\n",
    "dataset_test = datasets.ImageFolder('/Users/supearnesh/Downloads/chest_xray/test', transform=transform)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "loaders_cnn = {'train': loader_train, 'valid': loader_val, 'test': loader_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c54f8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following import is required for training to be robust to truncated images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # check if CUDA is available\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            # move to GPU if CUDA is available\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # check if CUDA is available\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## DONE: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print(f'The validation loss has decreased from {valid_loss_min} to {valid_loss}.')\n",
    "            valid_loss_min = valid_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "44d8f6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_01): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_01): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_02): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_02): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_03): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_03): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_04): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_04): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_05): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_05): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_01): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (fc_02): Linear(in_features=4096, out_features=133, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ## Choose an architecture\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        # convolutional layer (sees 1x256x364 tensor)\n",
    "        self.conv_01 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # batch normalization applied to convolutional layer\n",
    "        self.norm_01 = nn.BatchNorm2d(32)\n",
    "        # convolutional layer (sees 112x112x32 tensor)\n",
    "        self.conv_02 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # batch normalization applied to convolutional layer\n",
    "        self.norm_02 = nn.BatchNorm2d(64)\n",
    "        # convolutional layer (sees 56x56x64 tensor)\n",
    "        self.conv_03 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        # batch normalization applied to convolutional layer\n",
    "        self.norm_03 = nn.BatchNorm2d(128)\n",
    "        # convolutional layer pooled (sees 28x28x128 tensor)\n",
    "        self.conv_04 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        # batch normalization applied to convolutional layer\n",
    "        self.norm_04 = nn.BatchNorm2d(256)\n",
    "        # convolutional layer pooled (sees 7x7x256 tensor)\n",
    "        self.conv_05 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        # batch normalization applied to convolutional layer\n",
    "        self.norm_05 = nn.BatchNorm2d(512)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # linear layer (7 * 7 * 512 -> 500)\n",
    "        self.fc_01 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        # linear layer (4096 -> 133)\n",
    "        self.fc_02 = nn.Linear(4096, 133)\n",
    "        # dropout layer (p = 0.50)\n",
    "        self.dropout = nn.Dropout(0.50)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.norm_01(self.conv_01(x))))\n",
    "        x = self.pool(F.relu(self.norm_02(self.conv_02(x))))\n",
    "        x = self.pool(F.relu(self.norm_03(self.conv_03(x))))\n",
    "        x = self.pool(F.relu(self.norm_04(self.conv_04(x))))\n",
    "        x = self.pool(F.relu(self.norm_05(self.conv_05(x))))\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 7 * 7 * 512)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add first hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc_01(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add second hidden layer, with relu activation function\n",
    "        x = self.fc_02(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model_cnn = Net()\n",
    "print(model_cnn)\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1e078354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# select loss function\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move loss function to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    criterion_cnn = criterion_cnn.cuda()\n",
    "\n",
    "# select optimizer\n",
    "optimizer_cnn = optim.SGD(model_cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0910765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.005275 \tValidation Loss: 7.862362\n",
      "The validation loss has decreased from inf to 7.862361907958984.\n",
      "Epoch: 2 \tTraining Loss: 0.004499 \tValidation Loss: 7.202618\n",
      "The validation loss has decreased from 7.862361907958984 to 7.202617645263672.\n",
      "Epoch: 3 \tTraining Loss: 0.003026 \tValidation Loss: 7.116066\n",
      "The validation loss has decreased from 7.202617645263672 to 7.116065979003906.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/supearnesh/Downloads/chest_xray/train/pneumonia/person548_bacteria_2300.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-ca6e84da37d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model_cnn = train(n_epochs, loaders_cnn, model_cnn, optimizer_cnn, \n\u001b[0m\u001b[1;32m      5\u001b[0m                       criterion_cnn, use_cuda, 'model_cnn.pt')\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-16a51d4de6da>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# check if CUDA is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[1;32m    177\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/supearnesh/Downloads/chest_xray/train/pneumonia/person548_bacteria_2300.jpeg'"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "# train the model\n",
    "model_cnn = train(n_epochs, loaders_cnn, model_cnn, optimizer_cnn, \n",
    "                      criterion_cnn, use_cuda, 'model_cnn.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_cnn.load_state_dict(torch.load('model_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4beea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_cnn, model_cnn, criterion_cnn, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn = loaders_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn['train'].dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37856c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn = loaders_cnn\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = [item[4:].replace(\"_\", \" \") for item in data_cnn['train'].dataset.classes]\n",
    "\n",
    "def predict_xrays(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    global model_cnn\n",
    "    \n",
    "    # load image from file\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # all pre-trained models expect input images normalized in the same way\n",
    "    # i.e. mini-batches of 1-channel grayscale images of shape (1 x H x W)\n",
    "    # H and W are expected to be at least 224\n",
    "    \n",
    "    # the images have to be loaded in to a range of [0, 1]\n",
    "    # then normalized using mean = [0.60728765] and std = [0.10230779]\n",
    "    #transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor(), T.Normalize(mean=img_mean, std=img_std)])\n",
    "    \n",
    "    transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n",
    "    transformed_img = transform(img)\n",
    "    \n",
    "    print('transformed_img.shape: {}'.format(transformed_img.shape))\n",
    "    \n",
    "    # the images have to be loaded in to a range of [0, 1]\n",
    "    # then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "    normalize = T.Normalize(mean=img_mean, std=img_std)\n",
    "    normalized_img = normalize(transformed_img)\n",
    "    \n",
    "    print('normalized_img.shape: {}'.format(normalized_img.shape))\n",
    "    \n",
    "    tensor_img = tf.image.grayscale_to_rgb(normalized_img.unsqueeze(0))\n",
    "    \n",
    "    print('rgb_img.shape: {}'.format(rgb_img.shape))\n",
    "    \n",
    "    # model loading\n",
    "    #tensor_img = normalized_img.unsqueeze(0)\n",
    "    \n",
    "    print('tensor_img.shape: {}'.format(tensor_img.shape))\n",
    "    \n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # move image tensor to GPU if CUDA is available\n",
    "    if use_cuda:\n",
    "        tensor_img = tensor_img.cuda()\n",
    "    \n",
    "    # make prediction by passing image tensor to model\n",
    "    prediction = model_cnn(tensor_img)\n",
    "    \n",
    "    # move model prediction to GPU if CUDA is available\n",
    "    if use_cuda:\n",
    "        model_cnn = model_cnn.cuda()\n",
    "    \n",
    "    # convert predicted probabilities to class index\n",
    "    tensor_prediction = torch.argmax(prediction)\n",
    "    \n",
    "    # move prediction tensor to CPU if CUDA is available\n",
    "    if use_cuda:\n",
    "        tensor_prediction = tensor_prediction.cpu()\n",
    "    \n",
    "    predicted_class_index = int(np.squeeze(tensor_prediction.numpy()))\n",
    "    \n",
    "    return class_names[predicted_class_index] # predicted class index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f529c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70120faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = xray_files[10]\n",
    "\n",
    "predict_xrays(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7f2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
